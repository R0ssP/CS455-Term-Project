from pyspark.sql import SparkSession
import pyspark.sql.functions as F
from pyspark.sql.types import StructType, StructField, StringType, FloatType
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml import PipelineModel
from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator
import matplotlib.pyplot as plt
import numpy as np

import os
os.environ["PYSPARK_PYTHON"] = "/s/bach/j/under/jdy2003/miniconda3/bin/python3.12"

spark = SparkSession.builder.appName("analysis").getOrCreate()

# load data generated by model
crime_data = spark.read.csv("/user/jdy2003/nycFrame", header=True)

crime_predictions = spark.read.csv("/user/jdy2003/predictions", header=True)

zone_predictions = crime_predictions.groupBy("zone") \
    .agg(
        F.avg("prediction").alias("avg_prediction"),
        F.avg("response_time_in_minutes").alias("avg_response_time"),
        F.avg("Latitude").alias("avg_latitude"),
        F.avg("Longitude").alias("avg_longitude")
    )

best_zones = zone_predictions.orderBy(F.col("avg_prediction").asc())
worst_zones = zone_predictions.orderBy(F.col("avg_prediction").desc())

print("BEST PERFORMING ZONES")
best_zones.show()   
best_zones.write.mode("overwrite").option("header", "true").csv("/user/jdy2003/bestZones")
print("WORST PERFORMING ZONES")
worst_zones.show() 
worst_zones.write.mode("overwrite").option("header", "true").csv("/user/jdy2003/bestZones")


# BELOW IS THE AREA FOR METRICS MAE, R^2 RMSE FOR THE MODEL AND THE BASELINE!
evaluator_rmse = RegressionEvaluator(labelCol="response_time_in_minutes", predictionCol="prediction", metricName="rmse")
evaluator_mae = RegressionEvaluator(labelCol="response_time_in_minutes", predictionCol="prediction", metricName="mae")
evaluator_r2 = RegressionEvaluator(labelCol="response_time_in_minutes", predictionCol="prediction", metricName="r2")

crime_predictions = crime_predictions.withColumn("prediction", F.col("prediction").cast("float"))
crime_predictions = crime_predictions.withColumn("response_time_in_minutes", F.col("response_time_in_minutes").cast("float"))

rmse = evaluator_rmse.evaluate(crime_predictions)
mae = evaluator_mae.evaluate(crime_predictions)
r2 = evaluator_r2.evaluate(crime_predictions)

metric_path = "/s/bach/j/under/jdy2003/metrics.txt"

# find median and mean for le baseline
mean_response_time = crime_predictions.select(F.avg("response_time_in_minutes")).first()[0]
median_response_time = crime_predictions.select(F.expr("percentile_approx(response_time_in_minutes, 0.5)")).first()[0]

# shove em in
crime_predictions = crime_predictions.withColumn("mean_prediction", F.lit(mean_response_time))
crime_predictions = crime_predictions.withColumn("median_prediction", F.lit(median_response_time))

with open(metric_path, 'w') as file:
    file.write(f"Model Performance Metrics: \n")
    file.write(f"{"Root Mean Squared Error (RMSE):"} {rmse} \n")
    file.write(f"{"Mean Average Error (MAE): "} {mae} \n")
    file.write(f"{"R Squared Value: "} {r2} \n")

    file.write(f"Baseline Performance Metrics: \n")
    # use the evaluators to find baseline metrics!
    for col_name in ["mean_prediction", "median_prediction"]:
        rmse_b = evaluator_rmse.evaluate(crime_predictions, {evaluator_rmse.predictionCol: col_name})
        mae_b = evaluator_mae.evaluate(crime_predictions, {evaluator_mae.predictionCol: col_name})
        r2_b = evaluator_r2.evaluate(crime_predictions, {evaluator_r2.predictionCol: col_name})

        baseline_type = col_name.replace("_prediction","").capitalize()
        file.write(f"Baseline ({baseline_type}) - RMSE: {rmse_b}, MAE: {mae_b}, R^2: {r2_b}\n")
        print(f"Baseline ({col_name.replace('_prediction', '').capitalize()}) - RMSE: {rmse}, MAE: {mae}, R^2: {r2}")

print("Metrics written to file at ", metric_path)

# # BELOW IS FEATURE IMPORTANCE / CORRELATION
cast_columns = ['TAVG', 'PRCP', 'event_type_value', 'zone', 'DayOfYear',
                'response_time_in_minutes', 'Latitude', 'Longitude', 'accident_count']
for column in cast_columns:
    crime_predictions = crime_predictions.withColumn(column, F.col(column).cast("float"))

assembler = VectorAssembler(inputCols=["zone", "TAVG", "PRCP", "accident_count", "event_type_value"],
                            outputCol="features")

crime_predictions = assembler.transform(crime_predictions)

lr = LinearRegression(featuresCol="features", labelCol="response_time_in_minutes")

coefficients = crime_predictions.drop(F.col("prediction"))
model = lr.fit(coefficients)

coefficients = model.coefficients

file_path = "/s/bach/j/under/jdy2003/correlation.txt"

with open(file_path, 'w') as file:
    file.write("Feature Coefficients:\n")
    
    for i, col in enumerate(assembler.getInputCols()):
        file.write(f"{col}: {coefficients[i]}\n")

print("Feature coefficients have been written to ", file_path)

# # BELOW IS THE PREDICTION PLOT
predictions_pandas = crime_predictions.select("prediction", "response_time_in_minutes").toPandas() # gets the fields we want
plt.figure(figsize=(8, 6)) # 8 inches by 6 inches 
plt.scatter(predictions_pandas["prediction"], predictions_pandas["response_time_in_minutes"], alpha=0.3) # alpha = .5 is semitransparent markers
plt.xlabel("Predicted Response Time (minutes)")
plt.ylabel("Actual Response Time (minutes)")
plt.title("Scatter Plot: Predicted vs Actual Response Time")

predictions_path = "/s/bach/j/under/jdy2003/scatter_plot.png"
plot_path = os.path.join(predictions_path, predictions_path)
plt.savefig(plot_path, format='png')


#BELOW IS RESIDUAL ANALYSIS
crime_predictions = crime_predictions.withColumn("residuals", F.col("response_time_in_minutes") - F.col("prediction"))
residual_panda = crime_predictions.select("residuals").toPandas()
plt.figure(figsize=(8, 6))
plt.scatter(range(len(residual_panda)), residual_panda["residuals"], alpha=0.1)
plt.xlabel("Index")
plt.ylabel("Residuals")
residual_dir = "/s/bach/j/under/jdy2003/residual.png"
plt.savefig(residual_dir,format="png")


# #BELOW IS TIME SERIES ANALYSIS
time_series = crime_predictions.groupBy("DayOfYear").agg(F.avg("response_time_in_minutes").alias('avg_response_time'))
time_series_panda = time_series.toPandas()
plt.figure(figsize=(8,6))
plt.plot(time_series_panda["DayOfYear"],time_series_panda["avg_response_time"])
plt.xlabel("Day of Year")
plt.xticks(np.arange(0, max(time_series_panda["DayOfYear"]), 20))
plt.ylabel("Average Response Time (minutes)")
plt.title("Average Response Time Over Time")
time_series_dir = "/s/bach/j/under/jdy2003/time_series.png"
plt.savefig(time_series_dir, format="png")

spark.stop()